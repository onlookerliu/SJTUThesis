%# -*- coding: utf-8-unix -*-
% !TEX program = xelatex
% !TEX root = ../thesis.tex
% !TEX encoding = UTF-8 Unicode
%%==================================================
%% abstract.tex for SJTU Master Thesis
%%==================================================

\begin{abstract}

非线性系统最常见的损失函数即最小二乘，例如在神经网络的分类预测过程中，优化问题等价于在最小二乘损失下寻找最优的权重问题。这篇文章中，我们提出了一种改进的Levenberg-Marquardt方法用来训练这些系统，并成功应用在了人工(递归)神经网络中。相比已有的传统算法，我们证明了这种算法在满足局部误差的条件下，能够显著减少训练次数，并且在常见的数据集上提供了数值实验的对比。此外，我们还提出了一些调整超参数的策略。

\end{abstract}

\begin{englishabstract}

Error reduction is quite common in many nonlinear system, such as neural network training and solve nonlinear equations. Optimization task in particular scenario, is to find an optimal weight sets with delicate algorithms. In this paper, we proposed a modified Levenberg-Marquardt method for nonlinear system training and significantly reduce the least square error in our artificial neural networks. The theoretical basis for this method is given and the performance difference with respect to several other learning algorithms are shown on some well known data sets. The result indicates a much faster convergence rate under the local error bound condition. Also, some damping strategies are suggested to meet specific needs.

\end{englishabstract}

